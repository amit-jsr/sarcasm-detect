# -*- coding: utf-8 -*-
"""sarcasm_detection_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CgFV5Dx00LZDLj9NretYiRsTiLNsk96P
"""

!nvidia-smi

"""**Fine-Tune BERT for sarcasm dectection with TensorFlow**

## Task 2: Setup your TensorFlow and Colab Runtime

### Install TensorFlow and TensorFlow Model Garden
"""

import tensorflow as tf
print(tf.version.VERSION)

!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git

!pip install -q tf-models-official==2.4.0

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import sys
sys.path.append('models')
from official.nlp.data import classifier_data_lib
from official.nlp.bert import tokenization
from official.nlp import optimization

import os

import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

import tensorflow_hub as hub
import tensorflow_datasets as tfds
tfds.disable_progress_bar()

from official.modeling import tf_utils
from official import nlp
from official.nlp import bert

# Load the required submodules
import official.nlp.optimization
import official.nlp.bert.bert_models
import official.nlp.bert.configs
import official.nlp.bert.run_classifier
import official.nlp.bert.tokenization
import official.nlp.data.classifier_data_lib
import official.nlp.modeling.losses
import official.nlp.modeling.models
import official.nlp.modeling.networks

print("TF Version: ", tf.__version__)
print("Eager mode: ", tf.executing_eagerly())
print("Hub version: ", hub.__version__)
print("GPU is", "available" if tf.config.experimental.list_physical_devices("GPU") else "NOT AVAILABLE")

"""## Task 3: Download and Import Dataset"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/sarcasm/reddit/balanced_politics_subset.csv")
df.head()

df=df.sample(frac=1)

df.tail()

df['pair']=df['parent_comment']+str(" ")+df['comment']

df['pair'].replace('', np.nan, inplace=True)
df['pair'].replace('nan', np.nan, inplace=True)
df.dropna(subset=['pair'], inplace=True)

df['pair']=df['pair'].astype(str)

import re

def text_preprocessing(text):

    # Remove '@name'
    text = re.sub(r'(@.*?)[\s]', ' ', text)

    # Replace '&amp;' with '&'
    text = re.sub(r'&amp;', '&', text)

    # Remove trailing whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

df['pair']=df['pair'].apply(text_preprocessing)

"""## Task 4: Create tf.data.Datasets for Training and Evaluation"""

from sklearn.model_selection import train_test_split
train_df, remaining = train_test_split(df, random_state=42, train_size=0.80, stratify=df.label.values)
valid_df, test_df = train_test_split(remaining, random_state=42, train_size=0.50, stratify=remaining.label.values)

train_df.shape, valid_df.shape, test_df.shape

"""Train data for BERT"""

train_data = tf.data.Dataset.from_tensor_slices((train_df.pair.values, train_df.label.values))
valid_data = tf.data.Dataset.from_tensor_slices((valid_df.pair.values, valid_df.label.values))
#test_data = tf.data.Dataset.from_tensor_slices((test_df.comment.values, test_df.label.values))
for text, label in train_data.take(1):
  print(text)
  print(label)

"""## Task 5: Download a Pre-trained BERT Model from TensorFlow Hub"""

"""
Each line of the dataset is composed of the review text and its label
- Data preprocessing consists of transforming text to BERT input features:
input_word_ids, input_mask, segment_ids
- In the process, tokenizing the text is done with the provided BERT model tokenizer
"""

label_list = [0, 1] # Label categories
max_seq_length = 128 # maximum length of (token) input sequences
train_batch_size = 32

# Get BERT layer and tokenizer:
# More details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2",
                            trainable=True)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = bert.tokenization.FullTokenizer(vocab_file, do_lower_case)

tokenizer.convert_tokens_to_ids(["[CLS]","[SEP]"])

tokenizer.wordpiece_tokenizer.tokenize('hi, how are you doing?')

tokenizer.convert_tokens_to_ids(tokenizer.wordpiece_tokenizer.tokenize('hi, how are you doing?'))

"""## Task 6: Tokenize and Preprocess Text for BERT

<div align="center">
    <img width="300px" src='https://drive.google.com/uc?id=1-SpKFELnEvBMBqO7h3iypo8q9uUUo96P' />
    <p style="text-align: center;color:gray">BERT Tokenizer</p>
</div>

We'll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExamples using `classifier_data_lib`'s constructor `InputExample` provided in the BERT library.
"""

# This provides a function to convert row to input features and label

def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):
  example = classifier_data_lib.InputExample(guid = None,
                                            text_a = text.numpy(),
                                            text_b = None,
                                            label = label.numpy())
  feature = classifier_data_lib.convert_single_example(0, example, label_list,
                                    max_seq_length, tokenizer)

  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)

"""You want to use [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) to apply this function to each element of the dataset. [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) runs in graph mode.

- Graph tensors do not have a value.
- In graph mode you can only use TensorFlow Ops and functions.

So you can't `.map` this function directly: You need to wrap it in a [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function). The [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function.

## Task 7: Wrap a Python Function into a TensorFlow op for Eager Execution
"""

def to_feature_map(text, label):
  input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp=[text, label],
                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])

  # py_func doesn't set the shape of the returned tensors.
  input_ids.set_shape([max_seq_length])
  input_mask.set_shape([max_seq_length])
  segment_ids.set_shape([max_seq_length])
  label_id.set_shape([])

  x = {
        'input_word_ids': input_ids,
        'input_mask': input_mask,
        'input_type_ids': segment_ids
    }
  return (x, label_id)

"""## Task 8: Create a TensorFlow Input Pipeline with `tf.data`"""

with tf.device('/cpu:0'):
  # train
  train_data = (train_data.map(to_feature_map,
                              num_parallel_calls=tf.data.experimental.AUTOTUNE)
                          #.cache()
                          .shuffle(1000)
                          .batch(32, drop_remainder=True)
                          .prefetch(tf.data.experimental.AUTOTUNE))

  # valid
  valid_data = (valid_data.map(to_feature_map,
                            num_parallel_calls=tf.data.experimental.AUTOTUNE)
                          .batch(32, drop_remainder=True)
                          .prefetch(tf.data.experimental.AUTOTUNE))

"""The resulting `tf.data.Datasets` return `(features, labels)` pairs, as expected by [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit):"""

# data spec
train_data.element_spec

# data spec
valid_data.element_spec

"""## Task 9: Define Model"""

# Building the model
def create_model():
  #BERT
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_word_ids")
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_mask")
  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_type_ids")
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])


  #last4layers  = tf.keras.layers.Concatenate()([all_layer_output[-1], all_layer_output[-2], all_layer_output[-3], all_layer_output[-4]])

  net1 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')(sequence_output)
  net1 = tf.keras.layers.MaxPooling1D(4)(net1)

  net2 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')(sequence_output)
  net2 = tf.keras.layers.MaxPooling1D(4)(net2)

  net3 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')(sequence_output)
  net3 = tf.keras.layers.MaxPooling1D(4)(net3)

  net  = tf.keras.layers.Concatenate()([net1, net2, net3])

  net = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')(net)
  net = tf.keras.layers.MaxPooling1D(4)(net)

  #net = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')(net)
  #net = tf.keras.layers.MaxPooling1D(4)(net)


  net = tf.keras.layers.Flatten()(net)
 # net = tf.keras.layers.Dropout(0.1)(net)
  output = tf.keras.layers.Dense(1, activation="sigmoid", name="output")(net)


  model = tf.keras.Model(
    inputs={
        'input_word_ids': input_word_ids,
        'input_mask': input_mask,
        'input_type_ids': input_type_ids,
    },
    outputs=output)
  return model

"""###Fine-Tune BERT for Text Classification"""

model = create_model()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-6),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy()])
model.summary()

tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=60, )

"""### Evaluate the BERT Text Classification Model"""

from keras.callbacks import EarlyStopping, ModelCheckpoint
#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)
#es1 = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)

es = ModelCheckpoint("my_keras_model.h5", save_best_only=True)
es1 = EarlyStopping(patience=1, restore_best_weights=True)

# Train model
epochs = 5
history = model.fit(train_data,
                    validation_data=valid_data,
                    epochs=epochs,
                    verbose=1,callbacks=[es, es1])

"""### Plots"""

import matplotlib.pyplot as plt

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])
  plt.show()

plot_graphs(history, 'binary_accuracy')

plot_graphs(history, 'loss')

eval=model.evaluate(valid_data, verbose=1)

"""## Testing"""

test_data = tf.data.Dataset.from_tensor_slices((test_df.pair.values, [0]*test_df.shape[0]))
test_data = (test_data.map(to_feature_map).batch(1))

eval=model.evaluate(test_data, verbose=1)

eval

true_label=test_df['label']

true_label.shape

test_df.label.sum()

pred_label = model.predict(test_data)

pred_label.shape

type(pred_label)

pred=[1 if pred >=0.46 else 0 for pred in pred_label]

sum(pred)

from pandas import DataFrame
pred_df = DataFrame (pred,columns=['pred_label'])

from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score
precision_score(true_label,pred_df),recall_score(true_label,pred_df),f1_score(true_label,pred_df),accuracy_score(true_label,pred_df)

from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(true_label, pred_df)

def plot_roc_curve(fpr, tpr, label=None):
  plt.plot(fpr, tpr, linewidth=2, label=label)
  plt.plot([0, 1], [0, 1], 'k--')

plot_roc_curve(fpr, tpr)
plt.show()

from sklearn.metrics import roc_auc_score
roc_auc_score(true_label, pred_df)

"""## Model-2"""

# Building the model
def create_model():
  #BERT
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_word_ids")
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_mask")
  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_type_ids")
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])

  net1 = tf.keras.layers.Conv1D(filters=50, kernel_size=4, padding='valid', activation='relu')(sequence_output)
  net1 = tf.keras.layers.MaxPooling1D()(net1)

  net2 = tf.keras.layers.Conv1D(filters=50, kernel_size=4, padding='valid', activation='relu')(sequence_output)
  net2 = tf.keras.layers.MaxPooling1D()(net2)

  net3 = tf.keras.layers.Conv1D(filters=50, kernel_size=4, padding='valid', activation='relu')(sequence_output)
  net3 = tf.keras.layers.MaxPooling1D()(net3)

  net  = tf.keras.layers.Concatenate()([net1, net2, net3])

  net = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='valid', activation='relu')(net)
  net = tf.keras.layers.MaxPooling1D()(net)
  net = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='valid', activation='relu')(net)
  net = tf.keras.layers.MaxPooling1D()(net)

  #net = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')(net)
  #net = tf.keras.layers.MaxPooling1D(4)(net)


  net = tf.keras.layers.Flatten()(net)
 # net = tf.keras.layers.Dropout(0.1)(net)
  output = tf.keras.layers.Dense(1, activation="sigmoid", name="output")(net)


  model1 = tf.keras.Model(
    inputs={
        'input_word_ids': input_word_ids,
        'input_mask': input_mask,
        'input_type_ids': input_type_ids,
    },
    outputs=output)
  return model1

model1 = create_model()
model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-6),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy()])
model1.summary()

from keras.callbacks import EarlyStopping, ModelCheckpoint
#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)
#es1 = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)

es = ModelCheckpoint("my_keras_model.h5", save_best_only=True)
es1 = EarlyStopping(patience=1, restore_best_weights=True)

# Train model
epochs = 5
history = model1.fit(train_data,
                    validation_data=valid_data,
                    epochs=epochs,
                    verbose=1,callbacks=[es, es1])

"""### Testing"""

test_data = tf.data.Dataset.from_tensor_slices((test_df.pair.values, [0]*test_df.shape[0]))
test_data = (test_data.map(to_feature_map).batch(1))

eval=model1.evaluate(test_data, verbose=1)

eval

true_label=test_df['label']

true_label.shape

test_df.label.sum()

pred_label = model1.predict(test_data)

pred_label.shape

type(pred_label)

pred=[1 if pred >=0.46 else 0 for pred in pred_label]

sum(pred)

from pandas import DataFrame
pred_df = DataFrame (pred,columns=['pred_label'])

from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score
precision_score(true_label,pred_df),recall_score(true_label,pred_df),f1_score(true_label,pred_df),accuracy_score(true_label,pred_df)

from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(true_label, pred_df)

def plot_roc_curve(fpr, tpr, label=None):
  plt.plot(fpr, tpr, linewidth=2, label=label)
  plt.plot([0, 1], [0, 1], 'k--')

plot_roc_curve(fpr, tpr)
plt.show()

from sklearn.metrics import roc_auc_score
roc_auc_score(true_label, pred_df)